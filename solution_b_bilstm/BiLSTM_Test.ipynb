{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Input, Concatenate, Layer\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score,  recall_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Set a seed to decrease randomness\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set the glove file to refer to\n",
    "MODEL_NAME = 'glove.6B.100d'\n",
    "GLOVE_FILE = MODEL_NAME + '.txt'\n",
    "TOKENIZER_NAME = 'tokenizer_bilstm_' + MODEL_NAME + '.json'\n",
    "WEIGHTS_FILE = 'bilstm.'+ MODEL_NAME +'.weights.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean inputs\n",
    "def clean_input(document, char_filter = r\"[^\\w]\"):\n",
    "    \"\"\"\n",
    "    param document: original document\n",
    "    char_filter: regex specifying characters that need to be removed\n",
    "\n",
    "    return: cleaned document\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    # Goes through each sentence in the document\n",
    "    for sentence in document:\n",
    "\n",
    "        # convert all words to their lower case equivalent\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # tokenise\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        # join back words to get whole document\n",
    "        sentence = \" \".join(words)\n",
    "\n",
    "        # replace unwanted characters as specified by char_filter (default: non-word characters) with whitespace\n",
    "        sentence = re.sub(char_filter, \" \", sentence)\n",
    "\n",
    "        # replace multiple whitespaces with single whitespace\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "        # strip whitespace from document\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        # append the cleaned sentence to the new list\n",
    "        cleaned.append(sentence)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Training set\n",
    "premise_train = clean_input(train['premise'])\n",
    "hypothesis_train = clean_input(train['hypothesis'])\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "\n",
    "# Combine list of premises and hypothesis in training set and fit tokenizer on that\n",
    "combined = premise_train + hypothesis_train\n",
    "tokenizer.fit_on_texts(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open(GLOVE_FILE, encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "vocab_length = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create Embedding Matrix having n columns\n",
    "# Containing n-dimensional GloVe word embeddings for all words in our corpus.\n",
    "embedding_matrix = np.zeros((vocab_length, EMBEDDING_DIM))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an attention class\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        e = K.tanh(K.dot(inputs, self.W) + self.b)  # Compute attention scores\n",
    "        a = K.softmax(e, axis=1)  # Softmax over time axis\n",
    "        output = inputs * a  # Apply attention weights\n",
    "        return K.sum(output, axis=1)  # Weighted sum over time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " premise_input (InputLayer)  [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " hypothesis_input (InputLay  [(None, 100)]                0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 100, 100)             3307000   ['premise_input[0][0]',       \n",
      "                                                                     'hypothesis_input[0][0]']    \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 100, 128)             84480     ['embedding[0][0]',           \n",
      " al)                                                                 'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, 128)                  129       ['bidirectional[0][0]',       \n",
      "                                                                     'bidirectional[1][0]']       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 256)                  0         ['attention[0][0]',           \n",
      "                                                                     'attention[1][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   16448     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 64)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    65        ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3408122 (13.00 MB)\n",
      "Trainable params: 101122 (395.01 KB)\n",
      "Non-trainable params: 3307000 (12.62 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define Input Layers for Premise and Hypothesis\n",
    "premise_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\", name=\"premise_input\")\n",
    "hypothesis_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\", name=\"hypothesis_input\")\n",
    "\n",
    "# Shared Embedding Layer\n",
    "embedding_layer = Embedding(input_dim=vocab_length, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Encode Premise and Hypothesis\n",
    "premise_embedding = embedding_layer(premise_input)\n",
    "hypothesis_embedding = embedding_layer(hypothesis_input)\n",
    "\n",
    "# BiLSTM layers\n",
    "bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))\n",
    "premise_encoded = bilstm_layer(premise_embedding)\n",
    "hypothesis_encoded = bilstm_layer(hypothesis_embedding)\n",
    "\n",
    "# Attention layer\n",
    "attention = Attention()\n",
    "premise_attention = attention(premise_encoded)\n",
    "hypothesis_attention = attention(hypothesis_encoded)\n",
    "\n",
    "# Merge Representations (Use last hidden state)\n",
    "merged = Concatenate()([premise_attention, hypothesis_attention])\n",
    "\n",
    "# Fully Connected Layers\n",
    "dense = Dense(64, activation=\"relu\")(merged)\n",
    "dropout = Dropout(0.2)(dense)\n",
    "output = Dense(1, activation=\"sigmoid\")(dropout)  # Sigmoid for binary classification\n",
    "\n",
    "# Define and Compile Model\n",
    "model = Model(inputs=[premise_input, hypothesis_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 25 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 7s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "file_path = \"dev.csv\"\n",
    "test = pd.read_csv(file_path)\n",
    "\n",
    "# Clean inputs\n",
    "premise_test = clean_input(test['premise'])\n",
    "hypothesis_test = clean_input(test['hypothesis'])\n",
    "\n",
    "# Load trained model weights\n",
    "model.load_weights(WEIGHTS_FILE)\n",
    "\n",
    "# Open tokenizer file from json\n",
    "with open(TOKENIZER_NAME) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    premise_sequence_test = tokenizer.texts_to_sequences(premise_test)\n",
    "    hypothesis_sequence_test = tokenizer.texts_to_sequences(hypothesis_test)\n",
    "\n",
    "    # Pad sequences\n",
    "    premise_padded_test = pad_sequences(premise_sequence_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "    hypothesis_padded_test = pad_sequences(hypothesis_sequence_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict([premise_padded_test, hypothesis_padded_test])\n",
    "    predicted_labels = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    df_predictions = pd.DataFrame(predicted_labels, columns=['prediction'])\n",
    "    df_predictions.to_csv(\"Group_70_B.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7026\n",
      "F1 Score: 0.7019\n",
      "Precision: 0.6979\n",
      "Recall: 0.7026\n",
      "MCC: 0.4043\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(test[\"label\"], predicted_labels)\n",
    "f1 = f1_score(test[\"label\"], predicted_labels, average=\"weighted\")\n",
    "precision = precision_score(test[\"label\"], predicted_labels)\n",
    "recall = recall_score(test[\"label\"], predicted_labels, average=\"weighted\")\n",
    "mcc = matthews_corrcoef(test[\"label\"], predicted_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
