{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbxB0a8qTfmG"
      },
      "source": [
        "Imports and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI2VxGh-TFZo",
        "outputId": "550b4b93-467c-4d16-d466-6fedd3ad68cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Input, Concatenate, Layer\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vf1VcGK36KEj"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "EMBEDDING_DIM = 100\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Set a seed to decrease randomness\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Set the glove file to refer to\n",
        "MODEL_NAME = 'glove.6B.100d'\n",
        "GLOVE_FILE = MODEL_NAME + '.txt'\n",
        "TOKENIZER_NAME = 'tokenizer_bilstm_' + MODEL_NAME + '.json'\n",
        "WEIGHTS_FILE = 'bilstm.'+ MODEL_NAME +'.weights.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXu1MqojUjkm"
      },
      "source": [
        "Load CSV files and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wieDBE2nUUCI"
      },
      "outputs": [],
      "source": [
        "# Load CSV\n",
        "train = pd.read_csv('train.csv')\n",
        "valid = pd.read_csv('dev.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-celNowK6KEn"
      },
      "outputs": [],
      "source": [
        "# Clean inputs\n",
        "def clean_input(document, char_filter = r\"[^\\w]\"):\n",
        "    \"\"\"\n",
        "    param document: original document\n",
        "    char_filter: regex specifying characters that need to be removed\n",
        "\n",
        "    return: cleaned document\n",
        "    \"\"\"\n",
        "\n",
        "    cleaned = []\n",
        "\n",
        "    # Goes through each sentence in the document\n",
        "    for sentence in document:\n",
        "\n",
        "        # convert all words to their lower case equivalent\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        # tokenise\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # join back words to get whole document\n",
        "        sentence = \" \".join(words)\n",
        "\n",
        "        # replace unwanted characters as specified by char_filter (default: non-word characters) with whitespace\n",
        "        sentence = re.sub(char_filter, \" \", sentence)\n",
        "\n",
        "        # replace multiple whitespaces with single whitespace\n",
        "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
        "\n",
        "        # strip whitespace from document\n",
        "        sentence = sentence.strip()\n",
        "\n",
        "        # append the cleaned sentence to the new list\n",
        "        cleaned.append(sentence)\n",
        "\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AsYX9CXY6KEp"
      },
      "outputs": [],
      "source": [
        "# Identify subsets\n",
        "\n",
        "# Training set\n",
        "premise_train = clean_input(train['premise'])\n",
        "hypothesis_train = clean_input(train['hypothesis'])\n",
        "label_train = train['label']\n",
        "\n",
        "# Validation set\n",
        "premise_valid = clean_input(valid['premise'])\n",
        "hypothesis_valid = clean_input(valid['hypothesis'])\n",
        "label_valid = valid['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WwET4DTH6KEr"
      },
      "outputs": [],
      "source": [
        "# Initialize Tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "\n",
        "# Combine list of premises and hypothesis in training set and fit tokenizer on that\n",
        "combined = premise_train + hypothesis_train\n",
        "tokenizer.fit_on_texts(combined)\n",
        "\n",
        "# Save Tokenizer\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with io.open(TOKENIZER_NAME, 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "# Convert text to sequences\n",
        "premise_sequence_train = tokenizer.texts_to_sequences(premise_train)\n",
        "hypothesis_sequence_train = tokenizer.texts_to_sequences(hypothesis_train)\n",
        "premise_sequence_valid = tokenizer.texts_to_sequences(premise_valid)\n",
        "hypothesis_sequence_valid = tokenizer.texts_to_sequences(hypothesis_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q2MU5gl66KEt"
      },
      "outputs": [],
      "source": [
        "# Pad sequences\n",
        "premise_padded_train = pad_sequences(premise_sequence_train, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "hypothesis_padded_train = pad_sequences(hypothesis_sequence_train, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "premise_padded_valid = pad_sequences(premise_sequence_valid, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "hypothesis_padded_valid = pad_sequences(hypothesis_sequence_valid, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0sH_9cpY0ZT8"
      },
      "outputs": [],
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open(GLOVE_FILE, encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "vocab_length = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create Embedding Matrix having n columns\n",
        "# Containing n-dimensional GloVe word embeddings for all words in our corpus.\n",
        "embedding_matrix = np.zeros((vocab_length, EMBEDDING_DIM))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdJDjhs26KEu"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vEgJ7WYL6KEv"
      },
      "outputs": [],
      "source": [
        "# Defining an attention class\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        e = K.tanh(K.dot(inputs, self.W) + self.b)  # Compute attention scores\n",
        "        a = K.softmax(e, axis=1)  # Softmax over time axis\n",
        "        output = inputs * a  # Apply attention weights\n",
        "        return K.sum(output, axis=1)  # Weighted sum over time axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "tTCz5KAj6KEv",
        "outputId": "7e5994ab-e529-40f7-8f9a-43eeadb636a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " premise_input (InputLayer)  [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " hypothesis_input (InputLay  [(None, 100)]                0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 100)             3307000   ['premise_input[0][0]',       \n",
            "                                                                     'hypothesis_input[0][0]']    \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  (None, 100, 128)             84480     ['embedding[0][0]',           \n",
            " al)                                                                 'embedding[1][0]']           \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 128)                  129       ['bidirectional[0][0]',       \n",
            "                                                                     'bidirectional[1][0]']       \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 256)                  0         ['attention[0][0]',           \n",
            "                                                                     'attention[1][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 64)                   16448     ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 64)                   0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    65        ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3408122 (13.00 MB)\n",
            "Trainable params: 101122 (395.01 KB)\n",
            "Non-trainable params: 3307000 (12.62 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define Input Layers for Premise and Hypothesis\n",
        "premise_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\", name=\"premise_input\")\n",
        "hypothesis_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\", name=\"hypothesis_input\")\n",
        "\n",
        "# Shared Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=vocab_length, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            weights=[embedding_matrix], trainable=False)\n",
        "\n",
        "# Encode Premise and Hypothesis\n",
        "premise_embedding = embedding_layer(premise_input)\n",
        "hypothesis_embedding = embedding_layer(hypothesis_input)\n",
        "\n",
        "# BiLSTM layers\n",
        "bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))\n",
        "premise_encoded = bilstm_layer(premise_embedding)\n",
        "hypothesis_encoded = bilstm_layer(hypothesis_embedding)\n",
        "\n",
        "# Attention layer\n",
        "attention = Attention()\n",
        "premise_attention = attention(premise_encoded)\n",
        "hypothesis_attention = attention(hypothesis_encoded)\n",
        "\n",
        "# Merge Representations (Use last hidden state)\n",
        "merged = Concatenate()([premise_attention, hypothesis_attention])\n",
        "\n",
        "# Fully Connected Layers\n",
        "dense = Dense(64, activation=\"relu\")(merged)\n",
        "dropout = Dropout(0.2)(dense)\n",
        "output = Dense(1, activation=\"sigmoid\")(dropout)  # Sigmoid for binary classification\n",
        "\n",
        "# Define and Compile Model\n",
        "model = Model(inputs=[premise_input, hypothesis_input], outputs=output)\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3PzQGJ16KEw"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NNciosF0BoGG"
      },
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
        "\n",
        "checkpoint_filepath = WEIGHTS_FILE\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v29N-Z5Q6KEx",
        "outputId": "bb00610a-c9d4-40a4-919a-94486912429d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1527/1527 [==============================] - 78s 48ms/step - loss: 0.6413 - accuracy: 0.6241 - val_loss: 0.6072 - val_accuracy: 0.6620\n",
            "Epoch 2/10\n",
            "1527/1527 [==============================] - 72s 47ms/step - loss: 0.5960 - accuracy: 0.6704 - val_loss: 0.5896 - val_accuracy: 0.6755\n",
            "Epoch 3/10\n",
            "1527/1527 [==============================] - 76s 50ms/step - loss: 0.5718 - accuracy: 0.6929 - val_loss: 0.5701 - val_accuracy: 0.6927\n",
            "Epoch 4/10\n",
            "1527/1527 [==============================] - 72s 47ms/step - loss: 0.5482 - accuracy: 0.7128 - val_loss: 0.5694 - val_accuracy: 0.6918\n",
            "Epoch 5/10\n",
            "1527/1527 [==============================] - 72s 47ms/step - loss: 0.5242 - accuracy: 0.7267 - val_loss: 0.5667 - val_accuracy: 0.6961\n",
            "Epoch 6/10\n",
            "1527/1527 [==============================] - 72s 47ms/step - loss: 0.4970 - accuracy: 0.7488 - val_loss: 0.5733 - val_accuracy: 0.6934\n",
            "Epoch 7/10\n",
            "1527/1527 [==============================] - 72s 47ms/step - loss: 0.4688 - accuracy: 0.7647 - val_loss: 0.5902 - val_accuracy: 0.7026\n",
            "Epoch 8/10\n",
            "1527/1527 [==============================] - 71s 47ms/step - loss: 0.4394 - accuracy: 0.7845 - val_loss: 0.6152 - val_accuracy: 0.6948\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    [premise_padded_train, hypothesis_padded_train],\n",
        "    label_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_data=([premise_padded_valid, hypothesis_padded_valid], label_valid),\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stopping, model_checkpoint_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4bSg_M6KEx"
      },
      "source": [
        "Sample Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XWsFjWj6KEy",
        "outputId": "8d2c4c1e-0258-47c0-d124-c5c6c9600747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0.03178217]]\n",
            "Predicted Class: 0\n"
          ]
        }
      ],
      "source": [
        "# New Premise & Hypothesis\n",
        "new_premise = [\"A child is playing soccer\"]\n",
        "new_hypothesis = [\"An adult is not playing football\"]  # Likely entailment\n",
        "\n",
        "# Get model\n",
        "model.load_weights(WEIGHTS_FILE)\n",
        "\n",
        "# Open tokenizer file from json\n",
        "with open(TOKENIZER_NAME) as f:\n",
        "    data = json.load(f)\n",
        "    tokenizer = tokenizer_from_json(data)\n",
        "\n",
        "    # Convert to sequences\n",
        "    new_premise_seq = tokenizer.texts_to_sequences(new_premise)\n",
        "    new_hypothesis_seq = tokenizer.texts_to_sequences(new_hypothesis)\n",
        "\n",
        "    # Pad sequences\n",
        "    new_premise_padded = pad_sequences(new_premise_seq, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "    new_hypothesis_padded = pad_sequences(new_hypothesis_seq, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "\n",
        "    # Get Prediction\n",
        "    prediction = model.predict([new_premise_padded, new_hypothesis_padded])\n",
        "    print(prediction)\n",
        "\n",
        "    # Get Prediction Class\n",
        "    predicted_class = int(prediction[0] > 0.5)  # Convert probability to 0 or 1\n",
        "    print(\"Predicted Class:\", predicted_class)  # 0 = Contradiction, 1 = Entailment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s27Z4u06KEz"
      },
      "source": [
        "Evaluation by predicting validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gcLQ_Np6KEz",
        "outputId": "2258aa8e-0b1e-41be-f0af-1045f9a71652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "211/211 [==============================] - 6s 27ms/step\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "file_path = \"dev.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Load trained model\n",
        "model.load_weights(WEIGHTS_FILE)\n",
        "\n",
        "# Open tokenizer file from json\n",
        "with open(TOKENIZER_NAME) as f:\n",
        "    data = json.load(f)\n",
        "    tokenizer = tokenizer_from_json(data)\n",
        "\n",
        "    # Convert text to sequences\n",
        "    premise_sequences = tokenizer.texts_to_sequences(df[\"premise\"].tolist())\n",
        "    hypothesis_sequences = tokenizer.texts_to_sequences(df[\"hypothesis\"].tolist())\n",
        "\n",
        "    # Pad sequences\n",
        "    premise_padded = pad_sequences(premise_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "    hypothesis_padded = pad_sequences(hypothesis_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict([premise_padded_valid, hypothesis_padded_valid])\n",
        "    predicted_labels = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    df_predictions = pd.DataFrame(predicted_labels, columns=['prediction'])\n",
        "    df_predictions.to_csv(\"Group_70_B.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HtH3itB6KE0",
        "outputId": "5e6d0857-e92a-4a1b-ef32-47e91b89b96d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7026\n",
            "F1 Score: 0.7019\n",
            "Recall: 0.7026\n",
            "Entailment/Contradiction Ratio: 1.2386\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(df[\"label\"], predicted_labels)\n",
        "f1 = f1_score(df[\"label\"], predicted_labels, average=\"weighted\")\n",
        "recall = recall_score(df[\"label\"], predicted_labels, average=\"weighted\")\n",
        "\n",
        "# Entailment vs Contradiction Ratio\n",
        "entailment_count = (predicted_labels == 1).sum()\n",
        "contradiction_count = (predicted_labels == 0).sum()\n",
        "ratio = entailment_count / (contradiction_count + 1e-6)  # Avoid division by zero\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Entailment/Contradiction Ratio: {ratio:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
